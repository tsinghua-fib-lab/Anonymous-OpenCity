{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generativeAgent import GAGroup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# * Preparing Agents\n",
    "change the city as you wish\n",
    "\"\"\"\n",
    "ag = GAGroup(\"./config.yaml\", \"GA\", \"san_francisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# agents initialization\n",
    "arg_1: number of agents you want to simulate\n",
    "arg_2: Day\n",
    "arg_3: (Optional) if you have already sampled profile for agents, you can use this to accelerate the initialization, other wise, it will sample profiles according to the city\n",
    "\"\"\"\n",
    "city = \"san_francisco\"\n",
    "await ag.prepare_agents(1000, \"Sunday\", f\"prototype/{city}/agents_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "city = 'san_francisco'  # change city's name when you use\n",
    "# load agent profile from pre-stored files\n",
    "with open(f'prototype/{city}/protos.json', 'r') as f:\n",
    "    group_des = json.load(f)\n",
    "groups = {}\n",
    "for key, value in group_des.items():\n",
    "    groups[key] = []\n",
    "for i in range(len(ag.agents)):\n",
    "    groups[ag.agent_groups[i]].append(i)\n",
    "\n",
    "# get the actual agent group\n",
    "agent_groups = {}\n",
    "for proto, agent_ids in groups.items():\n",
    "    agent_groups[proto] = []\n",
    "    for id in agent_ids:\n",
    "        agent_groups[proto].append(ag.agents[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_groups(groups, noa):\n",
    "    \"\"\"if you want some of the agents to start simulation, you can use this sample function\"\"\"\n",
    "    nog = len(list(groups.keys()))\n",
    "    key_name = list(groups.keys())\n",
    "    noa_g = []\n",
    "    for key in key_name:\n",
    "        noa_g.append(len(groups[key]))\n",
    "    total = sum(noa_g)\n",
    "    propotion = []\n",
    "    for i in range(nog):\n",
    "        propotion.append(noa_g[i]/total)\n",
    "    propotion_n = [int(noa*propotion[i]) for i in range(len(propotion))]\n",
    "    if sum(propotion_n) < noa:\n",
    "        left = noa-sum(propotion_n)\n",
    "        index = 0\n",
    "        for i in range(left):\n",
    "            propotion_n[index] += 1\n",
    "            index = (index+1)%len(propotion_n)\n",
    "    new_groups = {}\n",
    "    index = 0\n",
    "    for key in key_name:\n",
    "        new_groups[key] = groups[key][:propotion_n[index]]\n",
    "        index += 1\n",
    "    return new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * OpenCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "CoT for Distill meta-prompt generation\n",
    "1. Conclusion and Summarization\n",
    "2. Context Share Cue\n",
    "3.1 With variable share cue\n",
    "3.2 With only context share cue\n",
    "\"\"\"\n",
    "def distill_meta_prompt_generate(llm, func_des, var_des, var_share, o_prompt) -> str:\n",
    "    len_o = len(o_prompt)\n",
    "    # step 1: Summary\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': f\"\"\"\n",
    "Part One: {func_des};\n",
    "Part Two:{var_des};\n",
    "Part Three:{o_prompt};\n",
    "\n",
    "The text above including 3 parts:\n",
    "Part One is the target of the original prompt; Part Two is the variable description in the original prompt; Part Three is the original prompt which is used to send to the request an answer after insert specific variable values. \n",
    "Can you sum up the the upper text? Include a summary and key points.\"\"\"\n",
    "    }]\n",
    "    summary = llm.text_request(messages)\n",
    "    messages.append({'role': 'assistant', 'content': summary})\n",
    "    \n",
    "    # step 2: Context Share Cue\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': \"Based on your analysis, if I have multiple requests use the same prompt shown abouv, what context information can be reused?\"})\n",
    "    can_share_context = llm.text_request(messages)\n",
    "    messages.append({'role': 'assistant', 'content': can_share_context})\n",
    "\n",
    "    if len(var_share) > 0:\n",
    "        # step 3: Variable Share Cue and Rebuild\n",
    "        messages.append({\n",
    "            'role': 'user',\n",
    "            'content': f\"\"\"\n",
    "Beside context, these variables {var_share} are identical across multiple requests.\n",
    "Can you rewrite the prompt with three sections?\n",
    "The first section is '## Shared Background', including shareable context and identical variables.\n",
    "The second section is '## Requriements', including the functionality and output format\n",
    "The third section is '## Situations', only remains a '!<INPUT-X>!' here.\n",
    "\n",
    "## Requirement\n",
    "1. For those variables that are identical, place it in the first section, replace other variables to a '!<INPUT-X>!' in third section, in which 'X' is 1 plus number of identical variables\n",
    "1. Your output only contains the rewrite version of Part Three.\n",
    "2. Remove as much honorary, repetitive information as possible.\n",
    "\n",
    "## Output Format\n",
    "You output is a string.\n",
    "\n",
    "Your output:\n",
    "\"\"\"})\n",
    "        rewrite = llm.text_request(messages)\n",
    "    else:\n",
    "        # step 4: Rebuild prompt\n",
    "        messages.append({\n",
    "            'role': 'user',\n",
    "            'content': f\"\"\"\n",
    "Can you rewrite the prompt with two sections?\n",
    "The first section is Shared Background, including shareable context and identical variables.\n",
    "The second part is Requests, including variables that can not be shared across multiple requests.\n",
    "\n",
    "## Requirement\n",
    "1. Your output only contains the rewrite version of Part Three.\n",
    "2. Remove as much honorary, repetitive information as possible.\n",
    "\n",
    "## Output Format\n",
    "You output is a string.\n",
    "\n",
    "Your output:\n",
    "\"\"\"})\n",
    "        rewrite = llm.text_request(messages)\n",
    "    return rewrite\n",
    "\n",
    "\n",
    "class BatchRunner:\n",
    "    def __init__(self, agents, prompt_dir, batch_dir:bool=False, max_size:int=None):\n",
    "        \"\"\"\n",
    "        agents: agents for the group\n",
    "        group_des: description of the agents for the group, profile unified description information\n",
    "        prompt_dir: path to the original prompt store used to generate the batch_prompt.\n",
    "        batch_dir: default None, if this is not None, the batch prompt will not be regenerated, but will reuse the prompt in this folder.\n",
    "        max_size: default None, indicates the size of the batch, if None means infinite.\n",
    "        \"\"\"\n",
    "        self.hub = False\n",
    "        self.prompts_raw = {}\n",
    "        self.prompt_vars = {}\n",
    "        self.prompt_des = {}\n",
    "        self.prompt_var_share = {}\n",
    "        self.share_pool = {}\n",
    "        self.prompts_batch = {}\n",
    "        self.num_group = 1\n",
    "        self.root_dir = prompt_dir\n",
    "        self.agents = agents\n",
    "        if self.agents != None and len(self.agents) > 0:\n",
    "            self.llm = self.agents[0].Soul\n",
    "        else:\n",
    "            self.llm = None\n",
    "        for item in os.listdir(prompt_dir):\n",
    "            file_path = os.path.join(prompt_dir, item)\n",
    "            file_name = item\n",
    "            if os.path.isfile(file_path):\n",
    "                ext = os.path.splitext(file_name)[1].lower()\n",
    "                if ext == '.json':\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        share_config = json.load(file)\n",
    "                    for var in share_config['share']:\n",
    "                        self.share_pool[var] = None\n",
    "                else:\n",
    "                    f = open(file_path, \"r\")\n",
    "                    name = file_name.split('.')[0]\n",
    "                    prompt_des, prompt_var, prompt_raw = f.read().split(\"<commentblockmarker>###</commentblockmarker>\")\n",
    "                    self.prompts_raw[name] = prompt_raw\n",
    "                    self.prompt_vars[name] = prompt_var\n",
    "                    self.prompt_des[name] = prompt_des\n",
    "                    f.close()\n",
    "                    \n",
    "        for key, value in self.prompt_vars.items():\n",
    "            self.prompt_var_share[key] = []\n",
    "            var_used = re.findall(r'--\\s*(\\w+)', value)\n",
    "            for var in var_used:\n",
    "                if var in self.share_pool.keys():\n",
    "                    self.prompt_var_share[key].append(var)\n",
    "                    \n",
    "        if max_size != None:\n",
    "            self.num_group = int(len(agents)/max_size)\n",
    "            if len(agents)%max_size != 0:\n",
    "                self.num_group += 1\n",
    "\n",
    "        if batch_dir:\n",
    "            # Load prebuild batch prompts\n",
    "            for dirpath, dirnames, filenames in os.walk(prompt_dir):\n",
    "                if \"batch_prompt\" in dirnames:\n",
    "                    target_folder_path = os.path.join(prompt_dir, \"batch_prompt\")\n",
    "                    for file in os.listdir(target_folder_path):\n",
    "                        name = file.split('.')[0]\n",
    "                        file_path = os.path.join(target_folder_path, file)\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            self.prompts_batch[name] = json.load(f)\n",
    "        else:\n",
    "            print(\"\"\"You have not set [batch_dir], default [root_dir/batch_prompt]:\n",
    "            1. You can run [generate_batch_prompt] to generate batch prompt for Batch group or \n",
    "            2. You can run [load_batch_prompt] to load exist batch prompt.\"\"\")\n",
    "\n",
    "    def generate_batch_prompt(self):\n",
    "        \"\"\"\n",
    "        Generate batch prompt using LLM-based method.\n",
    "        \"\"\"\n",
    "        print(\"Start Generation Batch Prompt...\")\n",
    "        for key, o_prompt in self.prompts_raw.items():\n",
    "            func_des = br.prompt_des[key]\n",
    "            var_des = br.prompt_vars[key]\n",
    "            var_share = br.prompt_var_share[key]\n",
    "            self.prompts_batch[key] = distill_meta_prompt_generate(self.llm, func_des, var_des, var_share, o_prompt)\n",
    "        print(\"Generation Success.\")\n",
    "\n",
    "    def load_batch_prompt(self, prompt_dir):\n",
    "        \"\"\"\n",
    "        Load exsit batch prompt\n",
    "        \"\"\"\n",
    "        for dirpath, dirnames, filenames in os.walk(prompt_dir):\n",
    "            if \"batch_prompt\" in dirnames:\n",
    "                target_folder_path = os.path.join(prompt_dir, \"batch_prompt\")\n",
    "                for file in os.listdir(target_folder_path):\n",
    "                    name = file.split('.')[0]\n",
    "                    file_path = os.path.join(target_folder_path, file)\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        self.prompts_batch[name] = json.load(f)\n",
    "        print(\"Load Batch Prompt Success.\")\n",
    "\n",
    "    def set_llm(self, llm_agent):\n",
    "        \"\"\"\n",
    "        Default, BatchRunner using the same llm agent as agents do, you can set a distinguish llm agent\n",
    "        The llm_agent has to support:\n",
    "        1. [.text_request(messages:dict)->string] for sync llm text request\n",
    "        2. [.atext_request(messages:dict)->string] for async llm text request\n",
    "        You can use [UrbanLLM] agent from pycityagent repo\n",
    "        \"\"\"\n",
    "        self.llm = llm_agent\n",
    "\n",
    "    def set_share_pool(self, target, value):\n",
    "        \"\"\"\n",
    "        Set value for share pool\n",
    "        \"\"\"\n",
    "        if target not in self.share_pool.keys():\n",
    "            print(f\"Error: no share entity nameed {target}\")\n",
    "        self.share_pool[target] = value\n",
    "\n",
    "    def save_batch_prompt(self, folder:str=None) -> None:\n",
    "        \"\"\"\n",
    "        save batch prompt into json files\n",
    "        default in [self.root_dir/batch_prompt]\n",
    "        if signed [folder], then save in [folder]\n",
    "        \"\"\"\n",
    "        if len(list(self.prompts_batch.keys())) == 0:\n",
    "            print(\"Nothing to save\")\n",
    "            return\n",
    "        else:\n",
    "            if folder == None:\n",
    "                save_folder = os.path.join(self.root_dir, 'batch_prompt')\n",
    "            else:\n",
    "                save_folder = folder\n",
    "            if not os.path.exists(save_folder):\n",
    "                print(f\"Creating subfolder [batch_prompt] in {self.root_dir}\")\n",
    "                os.makedirs(save_folder)\n",
    "            for key, value in self.prompts_batch.items():\n",
    "                with open(f\"{save_folder}/{key}.json\", \"w\") as file:\n",
    "                    json.dump(value, file, indent=4)\n",
    "\n",
    "    def to_batch_prompt_str(self, values:list, names:list):\n",
    "        result_str = \"\"\"\"\"\"\n",
    "        for i in range(len(self.agents)):\n",
    "            single_situation = \"\"\"\"\"\"\n",
    "            single_situation += f\"### Situation_{i+1}:\\n\"\n",
    "            for j in range(len(names)):\n",
    "                single_situation += f\"{names[j]}: {values[j][i]}\\n\"\n",
    "            single_situation += \"\\n\"\n",
    "            result_str += single_situation\n",
    "        return result_str\n",
    "\n",
    "    def gather_vars(self, target):\n",
    "        \"\"\"\n",
    "        gather variable values from agents, return a list group, for example:\n",
    "        \"\"\"\n",
    "        results_list = []\n",
    "        for agent in self.agents:\n",
    "            single = agent.Brain.Memory.Working.Reason[target]\n",
    "            results_list.append(single)\n",
    "        return results_list\n",
    "\n",
    "    def get_prompt(self, target, values:list):\n",
    "        \"\"\"\n",
    "        set values to target prompt and return the target prompt\n",
    "        \"\"\"\n",
    "        prompt = self.prompts_batch[target]\n",
    "        curr_input = [str(value) for value in values]\n",
    "        for i, value in enumerate(curr_input):  \n",
    "            prompt = prompt.replace(f\"!<INPUT {i}>!\", value)\n",
    "        return prompt\n",
    "\n",
    "    async def perceive(self):\n",
    "        for agent in self.agents:\n",
    "            pois = agent._simulator.map.query_pois(\n",
    "                center = (agent.motion['position']['xy_position']['x'], agent.motion['position']['xy_position']['y']), \n",
    "                radius = 1000,  \n",
    "                category_prefix= \"\", \n",
    "                limit = 20      # 限制了perceive的POI数量，防止过多\n",
    "            )\n",
    "            for poi, _ in pois:\n",
    "                agent.Brain.Memory.Working.Reason['spatial'].add_memory_from_poi(poi, \"You can see it from here.\", \"scene\")\n",
    "            agent.Brain.Memory.Working.Reason[\"perceived\"] = agent.Brain.Memory.Working.Reason['spatial'].to_str(\"all\")\n",
    "\n",
    "    async def generate_wake_up_hour(self):\n",
    "        # All share\n",
    "        commonset = self.share_pool['Commonset']\n",
    "        lifestyle = self.share_pool['Lifestyle']\n",
    "        day = self.share_pool['Day']\n",
    "        bprompt = self.get_prompt('wake_up_hour', [commonset, lifestyle, day])\n",
    "        sysprompt = \"Just answer a number x that refers to a hour of in 24-hour, no reasons needed.\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sysprompt},\n",
    "            {\"role\": \"user\", \"content\": bprompt}\n",
    "        ]\n",
    "        hours = await self.llm.atext_request(messages)\n",
    "        if '0' == hours[0]:\n",
    "            hours = hours[1:]\n",
    "        return int(eval(hours))\n",
    "\n",
    "    async def generate_daily_plan(self):\n",
    "        # All share\n",
    "        commonset = self.share_pool['Commonset']\n",
    "        lifestyle = self.share_pool['Lifestyle']\n",
    "        day = self.share_pool['Day']\n",
    "        wake_up_hour = self.share_pool['Wake_Up_Hour']\n",
    "        bprompt = self.get_prompt('daily_planning', [commonset, lifestyle, day, wake_up_hour])\n",
    "        messages = [{\"role\":\"user\", \"content\": bprompt}]\n",
    "        daily_plan = await self.llm.atext_request(messages)\n",
    "        return daily_plan\n",
    "\n",
    "    async def generate_action_from_plan(self):\n",
    "        def __checkPlace(loc_str, nxt):\n",
    "            loc_list = loc_str.split('\\n')\n",
    "            for loc in loc_list:\n",
    "                if nxt in loc:\n",
    "                    try:\n",
    "                        p = json.loads(loc[loc.find(':')+1:].replace('\\'', '\\\"'))\n",
    "                    except:\n",
    "                        p = None\n",
    "                    return p\n",
    "            return None\n",
    "\n",
    "        def sampleNoiseTime():\n",
    "            noise = random.randint(-10, 10)\n",
    "            return noise\n",
    "\n",
    "\n",
    "        def add_time(start_time, minutes):\n",
    "            \"\"\"\n",
    "            Calculate the time after the end, giving the start time and the incremental time (in minutes)\n",
    "            :param start_time: start time in the format '%H:%M'\n",
    "            :param minutes: incremental time in minutes\n",
    "            :return: the time after the end, in the format '%H:%M'; a flag for whether the day has been spanned\n",
    "            \"\"\"\n",
    "            start_datetime = datetime.strptime('2000-01-01 ' + start_time, '%Y-%m-%d %H:%M')\n",
    "            end_datetime = start_datetime + timedelta(minutes=minutes)\n",
    "            if end_datetime.day != start_datetime.day:\n",
    "                cross_day = True\n",
    "            else:\n",
    "                cross_day = False\n",
    "            end_time = end_datetime.strftime('%H:%M')\n",
    "            return end_time, cross_day\n",
    "\n",
    "        def getTimeFromZone(timeZone0):\n",
    "            time0, time1 = timeZone0.split('-')\n",
    "            time0 = float(time0)/2  # 这里已经化成小时了\n",
    "            time1 = float(time1)/2\n",
    "            sampleResult = random.uniform(time0, time1)  # 采样一个具体的时间值出来,单位是小时\n",
    "            minutes = int(sampleResult*60)\n",
    "            return minutes\n",
    "        \n",
    "        def sampleGapTime():\n",
    "            minutes = getTimeFromZone('0-2')  # 将事件的间隔时间设置为0到1个小时\n",
    "            return minutes\n",
    "\n",
    "        commonset = self.share_pool['Commonset']\n",
    "        daily_plan = self.share_pool['Daily_Plan']\n",
    "        current_time = self.share_pool['Current_Time']\n",
    "        # individual: memory, perceived\n",
    "        memory_list = self.gather_vars('memory')\n",
    "        perceived_list = self.gather_vars('perceived')\n",
    "        batch_prompt_str = self.to_batch_prompt_str([memory_list, perceived_list], [\"Memory\", \"Surrounding Places\"])\n",
    "        bprompt = self.get_prompt('action_planning', [commonset, daily_plan, current_time, batch_prompt_str])\n",
    "        messages = [{\"role\":\"user\", \"content\": bprompt}]\n",
    "        arrangements = []\n",
    "        nxt_locs = []\n",
    "        nxt_pois = []\n",
    "        hours = []\n",
    "        minutes = []\n",
    "        try:\n",
    "            response = await self.llm.atext_request(messages)\n",
    "            if \"```json\" in response:\n",
    "                response = json.loads(response.split('```json')[1].split('```')[0])\n",
    "            else:\n",
    "                response = json.loads(response)\n",
    "            if len(self.agents) != len(response):\n",
    "                raise Exception(\"Error number of response\")\n",
    "        \n",
    "            index = 0\n",
    "            for resp in response:\n",
    "                arrangement = resp['arrangement']\n",
    "                nxt_loc = resp['next_place']\n",
    "                nxt_poi = __checkPlace(perceived_list[index], nxt_loc)\n",
    "                hour = int(resp['hours'])\n",
    "                minute = int(resp['minutes'])\n",
    "                if nxt_poi is None: \n",
    "                    scence_length = len(self.agents[index].Brain.Memory.Working.Reason['spatial'].scene)\n",
    "                    cid = random.randint(0, scence_length-1)\n",
    "                    node_id = self.agents[index].Brain.Memory.Working.Reason['spatial'].scene[cid]\n",
    "                    nxt_poi = self.agents[index].Brain.Memory.Working.Reason['spatial'].spatial_dict[node_id]\n",
    "                arrangements.append(arrangement)\n",
    "                nxt_locs.append(nxt_loc)\n",
    "                nxt_pois.append(nxt_poi)\n",
    "                hours.append(hour)\n",
    "                minutes.append(minute)\n",
    "                index += 1\n",
    "        except Exception as err:\n",
    "            arrangements = []\n",
    "            nxt_locs = []\n",
    "            nxt_pois = []\n",
    "            hours = []\n",
    "            minutes = []\n",
    "            for i in range(len(self.agents)):\n",
    "                arrangements.append(\"go to this place\")\n",
    "                scence_length = len(self.agents[i].Brain.Memory.Working.Reason['spatial'].scene)\n",
    "                cid = random.randint(0, scence_length-1)\n",
    "                node_id = self.agents[i].Brain.Memory.Working.Reason['spatial'].scene[cid]\n",
    "                nxt_poi = self.agents[i].Brain.Memory.Working.Reason['spatial'].spatial_dict[node_id]\n",
    "                nxt_loc = nxt_poi['name']\n",
    "                hours.append(random.randint(1, 5))\n",
    "                minutes.append(random.randint(0, 60))\n",
    "                nxt_locs.append(nxt_loc)\n",
    "                nxt_pois.append(nxt_poi)\n",
    "\n",
    "        end_times = []\n",
    "        cross_days = []\n",
    "        for i in range(len(hours)):\n",
    "            increment_minutes = 60*hours[i]+minutes[i]\n",
    "            noiseTime = sampleNoiseTime()\n",
    "            if increment_minutes + noiseTime > 0:\n",
    "                increment_minutes = increment_minutes + noiseTime\n",
    "            end_time, cross_day = add_time(current_time, increment_minutes)\n",
    "            end_times.append(end_time)\n",
    "            cross_days.append(cross_day)\n",
    "        for i in range(len(self.agents)):\n",
    "            nxt_poi = nxt_pois[i]\n",
    "            current_intention = arrangements[i]\n",
    "            end_time = end_times[i]\n",
    "            cross_day = cross_days[i]\n",
    "            self.agents[i].Brain.Memory.Working.Reason['spatial'].add_memory_from_node(nxt_poi, f\"You went here for {current_intention} from {current_time} to {end_time}.\")\n",
    "            if cross_day or end_time == \"23:59\":\n",
    "                seTime = \"(\"+ current_time +\", 23:59)\"\n",
    "                self.agents[i].Brain.Memory.Working.Reason['history'].append([current_intention, seTime])\n",
    "            else:\n",
    "                seTime = \"(\"+ current_time+\", \"+end_time+\")\"\n",
    "                self.agents[i].Brain.Memory.Working.Reason['history'].append([current_intention, seTime])\n",
    "            self.agents[i].Brain.Memory.Working.Reason['intention'] = current_intention\n",
    "            count = len(self.agents[i].Brain.Memory.Working.Reason['memory'])\n",
    "            self.agents[i].Brain.Memory.Working.Reason['memory'].append(f\"({count}). {current_time}: you decide to {current_intention};\")\n",
    "            nextPlace = (nxt_poi['name'], nxt_poi['id'])\n",
    "            intent, setime = self.agents[i].Brain.Memory.Working.Reason['history'][-1]\n",
    "            thisThing = [intent, setime, nextPlace]\n",
    "            self.agents[i].Brain.Memory.Working.Reason['trajectory'].append(thisThing)\n",
    "            self.agents[i].Brain.Memory.Working.Reason['nowPlace'] = nextPlace\n",
    "            count = len(self.agents[i].Brain.Memory.Working.Reason['memory'])\n",
    "            self.agents[i].Brain.Memory.Working.Reason['memory'].append(f\"({count}). you go to {nextPlace[0]};\")\n",
    "\n",
    "        if cross_days[0] or end_times[0] == \"23:59\":\n",
    "            self.share_pool['Break'] = True\n",
    "        else:\n",
    "            gapTime = sampleGapTime()\n",
    "            tmpnowTime, cross_day = add_time(end_times[0], gapTime)  \n",
    "            if cross_day:\n",
    "                self.share_pool['Current_Time'] = end_times[0]\n",
    "            else:\n",
    "                self.share_pool['Current_Time'] = tmpnowTime\n",
    "            self.share_pool['Break'] = False\n",
    "        return None\n",
    "\n",
    "    async def memory_reflection(self):\n",
    "        if len(self.agents[0].Brain.Memory.Working.Reason['memory']) < 10:\n",
    "            return\n",
    "        memory_list = self.gather_vars('memory')\n",
    "        batch_prompt_str = self.to_batch_prompt_str([memory_list], [\"Memory\"])\n",
    "        bprompt = self.get_prompt('memory_reflection', [batch_prompt_str])\n",
    "        messages = [{\"role\":\"user\", \"content\": bprompt}]\n",
    "        response = await self.llm.atext_request(messages)\n",
    "        try:\n",
    "            response = json.loads(response.split('```json')[1].split('```')[0])\n",
    "            index = 0\n",
    "            for resp in response:\n",
    "                self.agents[index].Brain.Memory.Working.Reason['memory'] = [f\"(0). {resp}\"] + [f\"({i+1}). {m[m.find('.'):]}\" for i, m in enumerate(self.agents[index].Brain.Memory.Working.Reason['memory'][10:])]\n",
    "        except Exception as err:\n",
    "            pass\n",
    "    \n",
    "    async def run(self, log:bool=False):\n",
    "        \"\"\"main entrance\"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.Brain.Memory.Working.Reason['spatial'].clear_scene()\n",
    "        hour = await self.generate_wake_up_hour()\n",
    "        self.set_share_pool('Wake_Up_Hour', hour)\n",
    "        if int(hour) >= 10:\n",
    "            self.set_share_pool('Current_Time', f\"{hour}:00\")\n",
    "        else:\n",
    "            self.set_share_pool('Current_Time', f\"0{hour}:00\")\n",
    "        intent = \"Sleep at home\"\n",
    "        set_time = \"(\"+\"00:00\"+\", \"+self.share_pool['Current_Time']+\")\"\n",
    "        for agent in self.agents:\n",
    "            thisThing = [intent, set_time, agent.Brain.Memory.Working.Reason['Homeplace']]\n",
    "            agent.Brain.Memory.Working.Reason['trajectory'].append(thisThing)\n",
    "            agent.Brain.Memory.Working.Reason['history'].append([intent, set_time])\n",
    "            count = len(agent.Brain.Memory.Working.Reason['memory'])\n",
    "            agent.Brain.Memory.Working.Reason['memory'].append(f\"({count}). 00:00: you decide to sleep at {agent.Brain.Memory.Working.Reason['Homeplace'][0]};\")\n",
    "        \n",
    "        daily_plan = await self.generate_daily_plan()\n",
    "        self.set_share_pool('Daily_Plan', daily_plan)\n",
    "        for agent in self.agents:\n",
    "            count = len(agent.Brain.Memory.Working.Reason['memory'])\n",
    "            agent.Brain.Memory.Working.Reason['memory'].append(f\"({count}). {self.share_pool['Current_Time']}: you make the plan: {self.share_pool['Daily_Plan']};\")\n",
    "        while True:\n",
    "            for agent in self.agents:\n",
    "                agent.Brain.Memory.Working.Reason['spatial'].clear_scene()\n",
    "                \n",
    "            # perceive\n",
    "            await self.perceive()\n",
    "            # plan\n",
    "            await self.generate_action_from_plan()\n",
    "            # reflection\n",
    "            await self.memory_reflection()\n",
    "            \n",
    "            for agent in self.agents:\n",
    "                await agent.set_position_poi(agent.Brain.Memory.Working.Reason['nowPlace'][1], hub=self.hub)\n",
    "            if self.share_pool['Break']:\n",
    "                break\n",
    "        if log:\n",
    "            index = 0\n",
    "            for agent in self.agents:\n",
    "                print(f\"Agent{index}: {agent.Brain.Memory.Working.Reason['trajectory']}\")\n",
    "                index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch runner\n",
    "brs = []\n",
    "max_batch_size = 10\n",
    "for key, agents in agent_groups.items():\n",
    "    for agent in agents:\n",
    "        agent.Brain.Memory.Working.Reason['memory'] = []\n",
    "        agent.Brain.Memory.Working.Reason['trajectory'] = []\n",
    "    group_num = int(len(agents)/max_batch_size)\n",
    "    if len(agents)%max_batch_size != 0:\n",
    "        group_num += 1\n",
    "    for i in range(group_num):\n",
    "        if i == group_num-1:\n",
    "            br = BatchRunner(agents[i*max_batch_size:], \"./prompt_template/generative_agent\", batch_dir=True)\n",
    "        else:\n",
    "            br = BatchRunner(agents[i*max_batch_size:(i+1)*max_batch_size], \"./prompt_template/generative_agent\", batch_dir=True)\n",
    "        br.set_share_pool('Commonset', group_des[key])\n",
    "        br.set_share_pool('Lifestyle', \"It depends on weekday or weekend. When weekday people generally go to work and do some other things between work. It is important to note that people generally do not work on weekends and prefer entertainment, sports and leisure activities. There will also be more freedom in the allocation of time.\")\n",
    "        br.set_share_pool('Day', 'Sunday')\n",
    "        br.set_share_pool('Current_Time', '00:00')\n",
    "        br.set_share_pool('Break', False)\n",
    "        brs.append(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "print(\"Start simulation...\")\n",
    "start = time.time()\n",
    "tasks = [brs[i].run() for i in range(len(brs))]\n",
    "await asyncio.gather(*tasks)\n",
    "end = time.time()\n",
    "print(\"Finished simulation...\")\n",
    "print(f\"Time: {end-start:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\"\"\"For storage output\"\"\"\n",
    "def get_profile(agent):\n",
    "    gender = agent.Brain.Memory.Working.Reason['genderDescription'].split(':')[1].split(';')[0].strip()\n",
    "    educationDescription = agent.Brain.Memory.Working.Reason['educationDescription'].split(':')[1].split(';')[0].strip()\n",
    "    consumptionDescription = agent.Brain.Memory.Working.Reason['consumptionDescription'].split(':')[1].split(';')[0].strip()\n",
    "    occupationDescription = agent.Brain.Memory.Working.Reason['occupationDescription'].split(':')[1].split(';')[0].strip()\n",
    "    return [gender, educationDescription, consumptionDescription, occupationDescription]\n",
    "\n",
    "def get_output2folder(ag, output_folder, start_index:int=0):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    index = start_index\n",
    "    for agent in ag.agents:\n",
    "        # traj\n",
    "        user_id = ag.ids[index]\n",
    "        path = os.path.join(output_folder, f\"{user_id}_traj.json\")\n",
    "        with open(path, 'w') as json_file:\n",
    "            json.dump(agent.Brain.Memory.Working.Reason['trajectory'], json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # profile\n",
    "        profile = get_profile(agent)\n",
    "        path = os.path.join(output_folder, f\"{user_id}_profile.json\")\n",
    "        with open(path, 'w') as json_file:\n",
    "            json.dump(profile, json_file, indent=4, ensure_ascii=False)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'san_francisco'\n",
    "get_output2folder(ag, f'output/{city}/even_output_{len(ag.agents)}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "32d3aee4ab7ec6508c70b0e942f7f298edf3f51fd7e3a17751d37fe4b0c032cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
